Alright, we're starting now, so welcome to the interview. Let's begin with a simple question. Can you tell me a little bit about your background and what got you interested in ML Engineering? Yeah, so like right now I'm in third year of my college, I mean like I'm going to be in the sixth semester but yeah, from start of my college, I think from my second semester towards end of it I got interested in like machine learning. I, I got to know about machine learning from some channels, from YouTube channels and all and I thought yeah, it is an interesting field. I got to know some things about it. I took a course, a very famous course from Andrew Ng which is ML specialization course which is on Coursera. So in the break, in the summer break which I've got in the college, I actually completed that course and actually I was very intrigued by how like we do all the stuff, what we do in the like machine learning field. Right. So it was very interesting and like after that I was just doing some courses, then I got into deep learning, I then got into some nlp. I like, I learned NLP or like Transformers about all these things from some YouTube channel from a famous, very famous guy called like Andrej Karpathy, his famous, famous playlist on neural networks. That was amazing. So I built, I tried like I did some projects, I got into some hackathons with my friends and all, we worked on some ideas and all. So yeah, that was pretty amazing and that got me into this journey on ML and then DL and Genai stuff and that's how I thought that yeah, this is the field for me. I'm very interested in this field. I like, I like it is amazing what we do and I saw the possibilities that what we can build from this ever growing field. Right. So that's why I chose this field and that's why I'm in front of you interviewing for this role here. That's a great background and it's always awesome to hear how those initial courses and projects really hooked you in. Now let me dive a bit deeper into the technical side. So for the next question, imagine you're working on a machine learning project where you have a data set that isn't perfectly clean. How would you go about handling missing data or noisy data in that data set before you actually train your model? Yeah, so handling or cleaning the data set is obviously one of the like major parts of any, any ML project or whatever I'm doing. But it really depends on what type of data I'm working with. Right. So for some Data Actually missing data might be a new info. So you don't, you can't do like you, you don't have to remove it, but for some, for some like field or for some data you actually have to remove it. And it also depends on what algorithm I'm using, right? So if I'm, if I'm using, for example, for example, if I'm using linear regression, you, you don't want these like NAN values in your data set. So you, what we can do is what, we can simply fill them up with the mean values or like mode values. But. Those are not always the right choices. Right? And you can like, there are many like choices here about how, how would you clean the data? And if the, if the data is like, if the data is what if the data is like time series data, right. If the data is time series data, then you can fill the, fill the NN values with only the previous, previous time period value. Right. This is also a way or if you like. So there are multiple ways and if you want one single answer, it is a bit difficult to give one because it highly depends on which model are you using and what type of data you have got and what the data is related to which field. Right. There are multiple types of data, cross sectional data and panel data. So it highly depends what type of data you're working with, which model are we trying to use and accordingly, I'll try to clean the data. Yeah. Absolutely. That makes a lot of sense and I think you nailed it. It's all about the context of the data and the model you're using. There's definitely no one size fits all solution. So your approach of considering the type of data in the algorithm is spot on. All right, let's move on to another question. Suppose you're working on a project where you need to evaluate how well your ML model is performing. Could you walk me through how you would choose evaluation metrics and what metrics you might consider depending on the type of problem. Yeah, so this also depends on what like on what type of data or what type of application I'm working with. So for example, the most common one. So most common one you can say, for example, like some regression tasks. So you are actually predict, predicting some actual value about what actual value that variable could be. So for example, for housing prices, for example, you are making a model that predicts the housing prices, giving given some features. Right. So what you do to like evaluate how well your model is performing a simple metric, is that like a simple metrics, is that that we do is like we could like we calculate the cost and cost is nothing but cumulated errors, right? But that is not a very like good metric because initially when we got the data we like split the data into two parts. So we got the training data, not two parts. It actually it is three parts. So the training data, the dev data and then the test. Right? So the best way to like evaluate it, evaluate the model you are working with is to like first define these like metrics, these cost. So for example root mean square cost. Right? So it, it measures the overall error. I'm I'm talking about the regression task. So in this case what you do is that what we do while we, we are choosing the model, we try different models and we train the model and on the training set we try to minimize the cost as much as we, we can. Right? But the problem here is the training cost error if it is minimized, if it is too much minimized, then there is chances of overfitting. Then we test it in from the test data. And here is the thing. In the test data you want the data in the test should be a production quality data. I mean the data which, on which the model is going to like be perform like like on which the model is going to work on. So in the actual production what kind of data you will get and what kind of data you are expecting the model to do well on, right? You will put that data into the test data so test set and you will check the errors or root mean square error. How much error is the model giving on that set? So that is how you'll know that how well the model is generalized, right? So this is the, in the case of regression task, for example you get classification task. In the case of classification task it is very tough, not very tough. But the thing is you can't just check if it is correct or non correct, right? Because if the data set is skewed so you know the 70% of the data is like it is a classification task. So for example you are testing if the patient has a heart disease or not. And the data set actually has 70% patients which have like diseases and they have heart disease. So if the model just predicts all to. If the model just predicts that all have heart disease still it got, it gets 70% accuracy, right? So we don't want that. So so here we, what we do is that we like we calculate this confusion matrix and what this confusion matrix gives us is that we, what we do is that we calculate false positive like false negatives true positive, true negatives. And then we calculate recall precision and F1 score. So recall precision. These are very like important metrics, especially for the classification tasks. And they actually tell you, precision tells you how much, like how much correct answer is. It gives for the answer which were actually correct. Correct. Right. So these are some of the metrics that, that I would use. And there are all like. There are also multiple metrics, new metrics for multiple applications. You can actually, if I, if I'm like, like for certain applications, I can also like go through or custom, like I'll, I'll try to create some custom matrix, like matrix, not like matrix on which I'll test the system. So it also highly depends on what type of application I'm working with. Yeah. And what type of model I'm working with. Yeah. Absolutely. And that's a fantastic breakdown. You're spot on that the choice of evaluation metrics really depends on the type of problem you're tackling, whether it's regression, classification or something else. And it's great that you brought up things like the confusion matrix, precision, recall. Those are super critical in classification. And like you said, for regression you might go with something like RMSE or mae, but it all depends on what you're trying to measure and how you want to balance things like bias and variance. So yeah, that's a really solid approach.